# README


## Описание задачи

### Трек SberOnline

Задачу можно разделить на три части:

1. Разработать систему классификации отзывов пользователей в AppStore / Google Play:
по тексту отзыва определять ответственную команду
  * В виду отсутствия разметки для классификации - добавить возможность интерактивного взаимодействия с системой и ручной разметки
  * Производить сопоставление отзывов по похожей проблеме
2. Классификация тональности отзывов (баг, дефект, замечание, пожелание)


## Технические особенности

Расскажите про технологию вашего решения. Какие подходы используете? Почему именно их? Используете ли какие-либо дополнительные инструменты? Были ли альтернативы?

### Проблема №0 - Очистка датасета

- [ ] TO-DO Описание препроцессинга

### Проблема №1 - Отсутствие разметки для классификации по командам

#### Ключевые слова

Для того, чтобы иметь возможность сопоставлять команды и отзывы, мы решили представить их в одном **векторном пространстве**. Для этого для каждой команды был составлен список **ключевых слов**, описывающих зону ответственности этой команды. Массив ключевых слов, содержащихся во всем датасете отзывов были отсортированы по частоте встречаемости и из первой **1000** для команд были выбраны соответствующие слова.

При этом ключевые слова выбирались из отзыва и его названия, поскольку в последнем часто содержится основная информация об отзыве.

`"iOS Platform": ["touch id", "face id", "виджет избранное", "геокарты", "адресная книга", "механизм отправки событий аналитики", "аутентификация", "регистрация", "авторизация", "карта", "темная тема", "вход", "виджет", "обновление", "версия", "ios", "ipad", "вылет", "лагает", "тормозит", "айфон", "запуск", "загрузка", "отпечаток", "авторизаци", "зависание", "обнова", "глюк", "звук", "фейс", "долгая загрузка", "быстрый вход", "отображение", "iphone xr", "фэйс", "iphone 6", "пароль", "ввод пароля", "iphone x", "черная тема", "ошибка приложения", "iphone xs", "ipad pr"]`

Ключевые слова определялись при помощи двух методов: 
1. [TextRank](https://github.com/RaRe-Technologies/gensim) - Page Ranking подход для задачи суммаризации
2. [TermExtractor](https://github.com/igor-shevchenko/rutermextract) - подход, основанный на POS(part-of-speech) tagging

**Выводы**: После просмотра ключевых слов полученных в каждом подходе оказалось, что использование **TextRank** слишком опирается на претрейновые эмбеддинги и плохо подходит для наших текстов из специфической области, в то время как **TermExtract** опирается на данные из нашего корпуса текстов и выделяет осмысленные ключевые слова.

| Название+Отзыв      | TermExtract keywords            | Text Rank  keywords |
| ------------- |:------------------:| -----:|
| 'Приложение не открывается после обновления!!!! Уже неделю пытаюсь открыть ни у меня ни у моих друзей не получается'     | 'обновление', 'неделя', 'мои друзья', 'приложение' | 'пытаюсь' |
| Не могу скачать так как требует подключения Wi-fi     | 'подключение wi-fi' |   'требует' |

Код со сравнением находится в файле [3_baseline_oneHot.ipynb](https://github.com/korney3/SberCodeKeyIdea/blob/master/Code/3_baseline_oneHot.ipynb)

#### Предсказательные модели

1. **Baseline Keywords Matching**

В качестве бейзлайна использовалось предсказание ответственной за отзыв команды по взвешенному количеству пересечений ключевых слов в отзыве и в описании команды.

Полученные предсказание расположены в файле [team_labeled.pkl](https://github.com/korney3/SberCodeKeyIdea/blob/master/Resources/3_predictions/team_labeled.pkl)

2. **One-Hot vectorization**

Следующей итерацией был перевод отзывов и ключевых слов групп в вектора при помощи **One-Hot vectorization** метода. Предсказание команды производилось по двум метрикам - **euclidean distance** (по наименьшему расстоянию) и **cosine distance** (по наибольшему значению). 

Выводы: **euclidean distance** предсказывал команду с наименьшим числом ключевых слов для всех отзывов, поэтому в данном методе эта метрика не использовалась и выбор пал на **cosine distance**.

Файл с кодом для запуска модели расположен в [3_baseline_oneHot.ipynb](https://github.com/korney3/SberCodeKeyIdea/blob/master/Code/3_baseline_oneHot.ipynb), предсказания лежат в файле [onehot_cos_preds.csv](https://github.com/korney3/SberCodeKeyIdea/blob/master/Resources/3_predictions/tfidf_cos_preds.csv)

3. **Flair embeddings** and **BERT embeddings**

В качестве следующей модели были использованы претренированные эмбеддинги библиотеки [Flair](https://github.com/flairNLP/flair) на базе **FastText** для русского языка и [BERT](ttps://github.com/UKPLab/sentence-transformers). Среди всевозможных вариантов эмбеддингов Word2Vec были выбраны эти две, поскольку они были претренированы для экстракции эмбедингов из предложений.

Предсказания производились для двух метрик - **euclidean distance** (по наименьшему расстоянию) и **cosine distance** (по наибольшему значению).

Файл с кодом для запуска модели расположен в [5_embeddings.ipynb](https://github.com/korney3/SberCodeKeyIdea/blob/master/Code/5_embeddings.ipynb), предсказания лежат в файлах [teams_predictions_flair.csv](https://github.com/korney3/SberCodeKeyIdea/blob/master/Resources/3_predictions/teams_predictions_flair.csv)
 и [teams_predictions_bert.csv](https://github.com/korney3/SberCodeKeyIdea/blob/master/Resources/3_predictions/teams_predictions_bert.csv)
 
#### Выводы

Для полученных моделей были посчитаны метрики качества - **accuracy**, **recall**, **f1_score**.

Поскольку разметки для всех данных нет, из общего корпуса отзывов были случайно выбраны и размечены 400 отзывов. При этом в соответствие каждому отзыву ставилось сразу несколько классов, поскольку отзывы потенциально содердат в себе проблемы, относящиеся к разным командам.

Размеченные данные лежат в файле [marked.csv](https://github.com/korney3/SberCodeKeyIdea/blob/master/Resources/marked.csv)

![alt-текст](https://github.com/korney3/SberCodeKeyIdea/blob/master/Presentation%20materials/metrics.png)

1. Использованные нами подходы в основном превосходят результаты бейзлайна (**Keywords matching**)

2. Несмотря на то, что по средним значениям метрик наилучшие предсказания дает **One-Hot vectorization**, стандартное отклонение показывает, что для более грамотного сравнения этой модели и **Flair embeddings** and **BERT embeddings** нужно большее количество размеченных данных.

3. **Flair embeddings** and **BERT embeddings** позволяют **без переобучения модели** добавлять новые команды с их ключами и делать предсказания сразу же.

### Проблема №3 - Отсутствие разметки для классификации по тональности

- [ ] TO-DO Описание классификации по тональности

## Дополнительные ссылки

Что дополнительно почитать о вашем подходе? Опирались ли в своем решении на какие-то исследования, статьи, рекомендации? Приложите любые ссылки и оставьте к ним небольшой комментарий.

https://www.zora.uzh.ch/id/eprint/113425/1/C17.pdf

https://www.researchgate.net/publication/301442885_Bug_report_feature_request_or_simply_praise_On_automatically_classifying_app_reviews


## Контакты

Оставьте здесь ваши контакты для связи: социальные сети, мессенджеры и так далее.
